\documentclass[11pt]{article} % For LaTeX2e
\usepackage{rldmsubmit,palatino}
\usepackage{graphicx}
\usepackage{sidecap}
\usepackage{caption}
\usepackage{subcaption}

\title{Modular Inverse Reinforcement Learning on Human Motion}

\author{
Shun Zhang\\
Department of Computer Science\\
University of Texas at Austin\\
Austin, TX 78712 \\
\texttt{menie482@cs.utexas.edu} \\
\And
Matthew Tong \\
Center for Perceptual Systems\\
University of Texas at Austin\\
Austin, TX 78712 \\
\texttt{mhtong@gmail.com} \\
\AND
Mary Hayhoe \\
Center for Perceptual Systems\\
University of Texas at Austin\\
Austin, TX 78712 \\
\texttt{hayhoe@utexas.edu} \\
\And
Dana Ballard \\
Department of Computer Science\\
University of Texas at Austin\\
Austin, TX 78712 \\
\texttt{dana@cs.utexas.edu} \\
\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\commentd}[1]{{\bf **Dana: #1**}}

\begin{document}

\maketitle

%The abstract should be a maximum of 2000 characters of text, including
%spaces (no figure is allowed).
\begin{abstract}
Reinforcement learning has been seen as a useful model for understanding human
behavior because of the importance of the neural reward circuitry. However,
because of the difficulty of scaling up RL models to large state spaces, it has
been hard to apply these models to complex human behaviors. One potential
simplification, consistent with observations of natural behavior, is that
complex tasks can be broken down into independent sub-tasks, or modules . In
this paper, we use observed human behavior while walking along a path to
estimate the intrinsic reward values associated with different modular
sub-tasks. To do this we use a simplified version of Inverse Reinforcement
Learning to calculate the reward associated with path following, obstacle
avoidance, and target collection of humans acting in an immersive virtual
environment. Using the estimated values, a modular RL model can generate
realistic behavior consistent with human action choices. This provides a way of
understanding momentary sensorimotor decisions made in complex natural
environments.
\end{abstract}

\keywords{
Reinforcement learning, human motion, inverse learning
}

\startmain % to start the main 1-4 pages of the submission.

\include{content}

\bibliographystyle{plain}
\bibliography{paper}

\end{document}
